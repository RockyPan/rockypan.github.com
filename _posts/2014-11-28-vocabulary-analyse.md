阅读英文小说的一大障碍就是词汇量。但是在读某一本小说时，除掉非常普通的常用单词外，出于作者的用词习惯，小说的特定类型，有些词在小说中出现的频率会非常的高。这些词可以认为是这本小说的核心词汇。如果能把这些核心词汇事先掌握，那么读这本小说的时候会轻松得多。好过碰到一个生字就记一个生字，很多人包括我自己就是这么干的。

我以《傲慢与偏见》的英文版小说为例。我是在网上下载的，不版本的字数可能会不一样。

$ wc pride.txt
   12116  121887  697943 pride.txt

$ cat pride.txt | tr -cs A-Za-z '\n' | tr A-Z a-z | wc
  122736  122736  658618

全书共121887个单词。但为了计算统一，我们取第二方式得到的122736单词。差别在于第二方式下：I'm、 don't 这种会被拆开算成两个单词，还有good-looking这类合成词，所以算出的单词数会比直接用wc要多一些。

$ cat pride.txt | tr -cs A-Za-z '\n' | tr A-Z a-z | sort | uniq | wc
    6288    6288   53496

小说中使用的单词数为6288个。

$ cat pride.txt | tr -cs A-Za-z '\n' | tr A-Z a-z | sort | uniq -c | awk '{ if ($1 >= 5) {++count; total += $1 } } END { printf("count:%d total:%d\n", count, total) }'
count:2042 total:115327

上面的命令统计了出现次数大于5的单词的个数为2042个，共出现了115327次。115327/122736*100%＝93.96%，这2042个单词就占到了全书的差不多94%。

这2042个单词中有很多是很常见的，像is、a、the……。这自已维护了一个单词表，这个单词表中的单词是我已经100%掌握的了。再用这个单词表一过滤。最终剩下300多个单词。

这样我可以先有针对性的记一下这300来个单词，再看小说时就会顺畅的多，不会时不时被查生词打断。

我把这个写成了脚本，有兴趣的可以到这里下载。

共有三个文件，其中两个是脚本。ignores.txt是已经掌握的单词列表文件。

使用时用以下命令：

./word_list.sh proide.txt words.txt

第一个参数是输入文件，即待分析的小说文件，必须是纯文本文件。第二个参数是输出的单词列表文件。第二个参数可忽略，忽略后会使用输入文件后加.wlist后缀为输出文件名。

脚本在分析时会将在ignores.txt文件列出的单词忽略掉。你可以根据自己的情况将自己已经熟知的单词加入到ignores.txt文件中。使用add_ignores.sh脚本可以方便的往ignores.txt文件中添加单词。使用时直接将单词列表做为参数即可：

./add_ignores.sh the my is am a an do does did

单词列表能接受多长和系统环境有关，我没试过极限，一般一次加几十个没出过问题。添加的单词会自动去重不用担心重复添加。


